{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194ee92e-768b-4dfb-b655-6d2cdb41fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "%run tree_impl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd627557-49c4-41a0-ae8f-608576064eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 100.00%\n",
      "Decision Tree Structure:\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Do your reproducible split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42,     stratify=y\n",
    "\n",
    ")\n",
    "\n",
    "# 2) Instantiate your custom tree (no args needed)\n",
    "from tree_impl import DecisionTreeClassifierCustom, print_tree\n",
    "dt = DecisionTreeClassifierCustom()\n",
    "\n",
    "# 3) Fit on the training slice\n",
    "dt.fit(X_train, y_train, feature_names)\n",
    "print(\"Decision Tree Structure:\")\n",
    "# print_tree(dt.tree, dt.original_feature_names)\n",
    "\n",
    "# 4) Evaluate on test\n",
    "print(\"Test accuracy:\", (dt.predict(X_test) == y_test).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4395d02-e7e7-4cd1-943c-060091630594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Cell 1: One‑Hot Encode Train/Test Splits for Manhattan distance\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1) Reload raw data (strings) so we can one‑hot encode\n",
    "df_raw = pd.read_csv(\"data/mushroom_dataset.csv\")\n",
    "\n",
    "# 2) Create the same train/test split (stratified) on labels\n",
    "#    Using the y you already have from tree_impl, ensure it's the same order:\n",
    "X_all = df_raw.drop(columns=\"class\")\n",
    "y_all = df_raw[\"class\"]\n",
    "le = LabelEncoder().fit(df_raw[\"class\"])  # fits on all classes once\n",
    "y_all = le.transform(y_all)\n",
    "X_tr_raw, X_te_raw, y_train, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "# 3) One‑hot encode each split so distances are 0/1\n",
    "X_tr_oh = pd.get_dummies(X_tr_raw, columns=X_tr_raw.columns, drop_first=False)\n",
    "X_te_oh = pd.get_dummies(X_te_raw, columns=X_te_raw.columns, drop_first=False)\n",
    "\n",
    "# 4) Align columns (in case some rare category only appears in test)\n",
    "all_cols = sorted(set(X_tr_oh.columns).union(X_te_oh.columns))\n",
    "X_tr_oh = X_tr_oh.reindex(columns=all_cols, fill_value=0)\n",
    "X_te_oh = X_te_oh.reindex(columns=all_cols, fill_value=0)\n",
    "\n",
    "# 5) Grab numpy arrays & feature names\n",
    "X_train_oh = X_tr_oh.values\n",
    "X_test_oh  = X_te_oh.values\n",
    "feature_names_oh = all_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80aec666-ac14-41da-980a-4bf641ede5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       feature  pct_in_top5  mean_signed_weight\n",
      "0            gill-attachment_f   100.000000            0.583016\n",
      "1                 veil-color_w    59.166667           -0.000653\n",
      "2                ring-number_o    36.666667            0.000257\n",
      "3                 gill-color_n    24.166667            0.015249\n",
      "4                 gill-color_b    15.000000            0.018897\n",
      "..                         ...          ...                 ...\n",
      "61                stalk-root_e     0.833333            0.004677\n",
      "62  stalk-surface-above-ring_f     0.833333           -0.004816\n",
      "63                population_s     0.833333           -0.005291\n",
      "64                 gill-size_n     0.833333           -0.005469\n",
      "65                   habitat_m     0.833333           -0.005721\n",
      "\n",
      "[66 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Cell X: One‐hot + filtered LIME summary for Decision Tree\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from lime.lime_tabular      import LimeTabularExplainer\n",
    "from collections            import Counter, defaultdict\n",
    "from utils import text_utils\n",
    "\n",
    "# — your pre‑existing variables —\n",
    "# X_train_oh      : np.ndarray, one‐hot encoded training data (n_train × d)\n",
    "# X_test_oh       : np.ndarray, one‐hot encoded test data  (n_test  × d)\n",
    "# feature_names_oh: list of length d, the one‐hot column names\n",
    "# tree_classifier : your fitted DecisionTreeClassifierCustom\n",
    "# y_train         : train labels (0/1)\n",
    "# (make sure you’ve run the cells that create all of the above)\n",
    "\n",
    "# 1) Cluster medoids on the one‐hot test set\n",
    "kmed   = KMedoids(n_clusters=40, metric=\"manhattan\", random_state=0)\n",
    "kmed.fit(X_test_oh)\n",
    "rep_idx = kmed.medoid_indices_\n",
    "\n",
    "# 2) Define background & medoids\n",
    "bg = X_train_oh            # LIME’s background\n",
    "md = X_test_oh[rep_idx]    # the 40 held‐out medoids\n",
    "\n",
    "# 3) Mark every one‐hot column as categorical\n",
    "cat_feats = list(range(bg.shape[1]))\n",
    "\n",
    "# 4) A wrapper that maps one‐hot → label vector → tree predict_proba\n",
    "def dt_proba_oh(arr_oh):\n",
    "    arr_oh = np.atleast_2d(arr_oh)\n",
    "    # decode one‐hot to label‐encoded integers\n",
    "    from tree_impl import label_encoders, feature_names\n",
    "    X_le = []\n",
    "    for row in arr_oh:\n",
    "        decoded = []\n",
    "        idx = 0\n",
    "        for feat in feature_names:\n",
    "            le = label_encoders[feat]\n",
    "            n_cats = len(le.classes_)\n",
    "            block = row[idx:idx+n_cats]\n",
    "            decoded.append(int(np.argmax(block)))\n",
    "            idx += n_cats\n",
    "        X_le.append(decoded)\n",
    "    X_le = np.array(X_le)\n",
    "    # predict and build a 2‐col proba array\n",
    "    preds = tree_classifier.predict(X_le)\n",
    "    proba = np.zeros((len(preds), 2))\n",
    "    for i,p in enumerate(preds):\n",
    "        proba[i, int(p)] = 1\n",
    "    return proba\n",
    "\n",
    "# 5) Run LIME on each medoid × 3 seeds, filter out dummy=0  \n",
    "agg_freq, agg_wt = Counter(), defaultdict(float)\n",
    "seeds = [0,1,2]\n",
    "for seed in seeds:\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data         = bg,\n",
    "        feature_names         = feature_names_oh,\n",
    "        class_names           = [\"edible\",\"poisonous\"],\n",
    "        categorical_features  = cat_feats,\n",
    "        discretize_continuous = False,\n",
    "        random_state          = seed\n",
    "    )\n",
    "    for row in md:\n",
    "        exp = explainer.explain_instance(\n",
    "            row,\n",
    "            dt_proba_oh,\n",
    "            num_features=bg.shape[1]   # ask for all, so we can filter safely\n",
    "        )\n",
    "        raw = exp.as_list(label=np.argmax(dt_proba_oh(row.reshape(1,-1))[0]))\n",
    "        count = 0\n",
    "        for feat_val, wt in raw:\n",
    "            feat = feat_val.split(\"=\",1)[0]        # strip “=0” or “=1”\n",
    "            idx  = feature_names_oh.index(feat)\n",
    "            if row[idx] == 1:                      # only keep actual 1’s\n",
    "                agg_freq[feat] += 1\n",
    "                agg_wt[feat]   += wt\n",
    "                count += 1\n",
    "                if count == 5:                     # top 5 only\n",
    "                    break\n",
    "\n",
    "# 6) Build and print the summary DataFrame\n",
    "k_total = len(md) * len(seeds)  # 40 × 3 = 120 runs\n",
    "rows = []\n",
    "for feat, freq in agg_freq.items():\n",
    "    rows.append({\n",
    "        \"feature\":            feat,\n",
    "        \"pct_in_top5\":       100 * freq      / k_total,\n",
    "        \"mean_signed_weight\": agg_wt[feat]   / freq\n",
    "    })\n",
    "summary_df = pd.DataFrame(rows).sort_values(\n",
    "    [\"pct_in_top5\",\"mean_signed_weight\"],\n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(summary_df)\n",
    "text_utils.ensure_directory_exists(\"eval/lime_results\")\n",
    "summary_df.to_csv(\"eval/lime_results/lime_dt_summary_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212a3c6-541f-4aca-86cd-1809be42610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
