{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5307ddd1-c4a0-4da0-8cf1-6145260b3b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 2.04228\n",
      "example:  predicted tensor([[0.4798, 0.5202],\n",
      "        [0.4821, 0.5179],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.4547, 0.5453],\n",
      "        [0.4481, 0.5519]]) , actual tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "------------------------------\n",
      "[2,    30] loss: 1.96896\n",
      "example:  predicted tensor([[0.5452, 0.4548],\n",
      "        [0.4842, 0.5158],\n",
      "        [0.4761, 0.5239],\n",
      "        [0.4833, 0.5167],\n",
      "        [0.5355, 0.4645]]) , actual tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]]) \n",
      "\n",
      "------------------------------\n",
      "[3,    30] loss: 1.88677\n",
      "example:  predicted tensor([[0.4438, 0.5562],\n",
      "        [0.4223, 0.5777],\n",
      "        [0.4437, 0.5563],\n",
      "        [0.5243, 0.4757],\n",
      "        [0.4294, 0.5706]]) , actual tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "------------------------------\n",
      "[4,    30] loss: 1.76487\n",
      "example:  predicted tensor([[0.3521, 0.6479],\n",
      "        [0.3953, 0.6047],\n",
      "        [0.3703, 0.6297],\n",
      "        [0.5182, 0.4818],\n",
      "        [0.4070, 0.5930]]) , actual tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "------------------------------\n",
      "[5,    30] loss: 1.63522\n",
      "example:  predicted tensor([[0.4077, 0.5923],\n",
      "        [0.3868, 0.6132],\n",
      "        [0.3266, 0.6734],\n",
      "        [0.6444, 0.3556],\n",
      "        [0.4106, 0.5894]]) , actual tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "------------------------------\n",
      "sample: tensor([[0.6348, 0.3652],\n",
      "        [0.4158, 0.5842],\n",
      "        [0.3713, 0.6287],\n",
      "        [0.4547, 0.5453],\n",
      "        [0.3695, 0.6305]], grad_fn=<SliceBackward0>) tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "training: 0.9262754753111749\n",
      "sample: tensor([[0.4540, 0.5460],\n",
      "        [0.4463, 0.5537],\n",
      "        [0.4275, 0.5725],\n",
      "        [0.3596, 0.6404],\n",
      "        [0.3908, 0.6092]], grad_fn=<SliceBackward0>) tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "test: 0.929889298892989\n",
      "sample: tensor([[0.4687, 0.5313],\n",
      "        [0.4213, 0.5787],\n",
      "        [0.3896, 0.6104],\n",
      "        [0.4259, 0.5741],\n",
      "        [0.3736, 0.6264]], grad_fn=<SliceBackward0>) tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "prob: 0.9266371245691778\n",
      "sample: tensor([[0.4687, 0.5313],\n",
      "        [0.4213, 0.5787],\n",
      "        [0.3896, 0.6104],\n",
      "        [0.4259, 0.5741],\n",
      "        [0.3736, 0.6264]], grad_fn=<SliceBackward0>) tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "test: 0.9266371245691778\n"
     ]
    }
   ],
   "source": [
    "%run models/neural_nets.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cadbad0-5b17-42fd-8362-827b527b13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One‑hot shape: (8124, 117)\n",
      "Features: 117\n",
      "Train/test sizes: 5686 2438\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"mushroom_dataset.csv\")\n",
    "\n",
    "# 2) Extract raw X and y\n",
    "X_raw = df.drop(columns=\"class\")\n",
    "y_raw = df[\"class\"]\n",
    "\n",
    "# 3) Label‑encode the target\n",
    "le = LabelEncoder().fit(y_raw)\n",
    "y = le.transform(y_raw)    # 0='e', 1='p'\n",
    "\n",
    "# 4) One‑hot encode all features\n",
    "X_oh_df = pd.get_dummies(X_raw, drop_first=False)\n",
    "feature_names_oh = X_oh_df.columns.tolist()\n",
    "X_oh = X_oh_df.values      # convert to numpy\n",
    "\n",
    "# 5) Stratified train/test split\n",
    "X_train_oh, X_test_oh, y_train, y_test = train_test_split(\n",
    "    X_oh, y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"One‑hot shape:\", X_oh.shape)\n",
    "print(\"Features:\", len(feature_names_oh))\n",
    "print(\"Train/test sizes:\", X_train_oh.shape[0], X_test_oh.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d37eed-2bf5-4ee2-9fe3-3ce59a4c74b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 40 medoids from test set\n",
      "Example medoid indices: [ 736  589 2233 1180    3 1549 1930 1764 2187  514]\n",
      "Medoids shape: (40, 117)\n"
     ]
    }
   ],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# 6) Find 40 medoids on the ONE‑HOT test set\n",
    "kmed = KMedoids(n_clusters=40, metric=\"manhattan\", random_state=0)\n",
    "kmed.fit(X_test_oh)\n",
    "\n",
    "# medoid indices into X_test_oh\n",
    "med_idx    = kmed.medoid_indices_\n",
    "# the actual medoid rows (one‑hot)\n",
    "medoids_oh = X_test_oh[med_idx]\n",
    "\n",
    "print(f\"Selected {len(med_idx)} medoids from test set\")\n",
    "print(\"Example medoid indices:\", med_idx[:10])\n",
    "print(\"Medoids shape:\", medoids_oh.shape)  # should be (40, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb99cd10-923a-4c45-bd74-2d1203ee973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def nn_proba_oh(arr_oh):\n",
    "    \"\"\"\n",
    "    1) arr_oh: 1‑D or 2‑D numpy of one‑hot dummies\n",
    "    2) convert each sample back to label‑encoded ints\n",
    "    3) run through your NN (which expects integer codes + embeddings)\n",
    "    4) return a (n_samples × 2) numpy array of softmax probs\n",
    "    \"\"\"\n",
    "    arr = np.atleast_2d(arr_oh)\n",
    "    # 1: convert one‑hot → label‑encoded vector\n",
    "    X_le_batch = np.stack([onehot_to_label_vect(r) for r in arr])\n",
    "    # 2: to torch.LongTensor\n",
    "    t = torch.from_numpy(X_le_batch).long()\n",
    "    # 3: forward pass + softmax\n",
    "    with torch.no_grad():\n",
    "        logits = model(t)\n",
    "        probs  = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcd165d-9a8c-4066-89e0-b050cf8d0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data         = X_train_oh,\n",
    "    feature_names         = feature_names_oh,\n",
    "    class_names           = [\"edible\",\"poisonous\"],\n",
    "    categorical_features  = list(range(X_train_oh.shape[1])),\n",
    "    discretize_continuous = False,\n",
    "    random_state          = 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16c2c43-11ae-4de1-b62b-94b9304d7a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoders for: ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor'] … total: 22\n",
      "Label‐encoded vector for medoid #0: [2 3 3 1 5 1 0 0 7 1 1 2 2 7 7 0 2 1 4 2 4 0]\n",
      "NN predict_proba on medoid #0: [[0.5230687  0.47693124]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# — you should already have these from the previous cell —\n",
    "# X_raw            : DataFrame of original string‐valued features\n",
    "# feature_names_oh : list of one‐hot column names (e.g. \"odor_n\", \"odor_f\", …)\n",
    "# medoids_oh       : numpy array of shape (40, n_dummies)\n",
    "# encoders         : dict mapping each original feature → its sklearn LabelEncoder\n",
    "# 1) Rebuild the encoders for each original feature\n",
    "encoders = {}\n",
    "for feat in X_raw.columns:\n",
    "    le = LabelEncoder().fit(X_raw[feat])\n",
    "    encoders[feat] = le\n",
    "\n",
    "# Quick check\n",
    "print(\"Saved encoders for:\", list(encoders.keys())[:5], \"… total:\", len(encoders))\n",
    "# 1) Build a mapping from each raw feature to its dummy columns\n",
    "orig_feats = X_raw.columns.tolist()   # same order you used for get_dummies\n",
    "feat_to_cols = {\n",
    "    feat: [col for col in feature_names_oh if col.startswith(feat + \"_\")]\n",
    "    for feat in orig_feats\n",
    "}\n",
    "\n",
    "# 2) Conversion function\n",
    "def onehot_to_label_vect(row_oh):\n",
    "    \"\"\"\n",
    "    row_oh: 1-D numpy array of length == len(feature_names_oh)\n",
    "    Returns: 1-D numpy int‐array of length == len(orig_feats)\n",
    "    \"\"\"\n",
    "    x_le = []\n",
    "    for feat in orig_feats:\n",
    "        cols = feat_to_cols[feat]\n",
    "        # grab that block of dummy values\n",
    "        idxs  = [feature_names_oh.index(c) for c in cols]\n",
    "        block = row_oh[idxs]\n",
    "        # choose the dummy==1 (argmax over 0/1)\n",
    "        k     = int(np.argmax(block))\n",
    "        # parse category letter from the dummy name\n",
    "        cat   = cols[k].split(\"_\", 1)[1]\n",
    "        # turn it back into the integer code\n",
    "        code  = encoders[feat].transform([cat])[0]\n",
    "        x_le.append(code)\n",
    "    return np.array(x_le, dtype=np.int64)\n",
    "\n",
    "# 3) Test it on the first medoid\n",
    "test_row_le = onehot_to_label_vect(medoids_oh[0])\n",
    "print(\"Label‐encoded vector for medoid #0:\", test_row_le)\n",
    "\n",
    "# 4) Sanity‐check: run it through your NN wrapper\n",
    "proba = nn_proba_oh(medoids_oh[0])\n",
    "print(\"NN predict_proba on medoid #0:\", proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4061672-a24f-43e2-8695-b11723421efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       feature  pct_in_top5  mean_signed_weight\n",
      "0   stalk-surface-above-ring_s    70.000000            0.011636\n",
      "1                       odor_n    62.500000            0.038329\n",
      "2     stalk-color-below-ring_w    46.666667            0.011111\n",
      "3          spore-print-color_k    37.500000            0.022034\n",
      "4                 population_v    33.333333           -0.010626\n",
      "5          spore-print-color_n    30.000000            0.027900\n",
      "6                 gill-color_n    25.000000            0.029302\n",
      "7     stalk-color-above-ring_p    20.000000           -0.013371\n",
      "8                       odor_f    20.000000           -0.064815\n",
      "9                 gill-color_w    17.500000            0.015621\n",
      "10               ring-number_o    17.500000           -0.010365\n",
      "11  stalk-surface-below-ring_k    17.500000           -0.016537\n",
      "12         spore-print-color_w    17.500000           -0.017658\n",
      "13  stalk-surface-above-ring_k    17.500000           -0.040579\n",
      "14                veil-color_w    16.666667            0.011431\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import text_utils\n",
    "\n",
    "agg_freq, agg_wt = Counter(), defaultdict(float)\n",
    "seeds = [0,1,2]\n",
    "d = X_train_oh.shape[1]\n",
    "cat_feats = list(range(d))\n",
    "\n",
    "for seed in seeds:\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data         = X_train_oh,\n",
    "        feature_names         = feature_names_oh,\n",
    "        class_names           = [\"edible\",\"poisonous\"],\n",
    "        categorical_features  = cat_feats,\n",
    "        discretize_continuous = False,\n",
    "        random_state          = seed\n",
    "    )\n",
    "    for row in medoids_oh:\n",
    "        # build the explanation (no label arg needed)\n",
    "        exp = explainer.explain_instance(\n",
    "            row,\n",
    "            nn_proba_oh,\n",
    "            num_features=d\n",
    "        )\n",
    "\n",
    "        # Grab the single key in exp.as_map(): the class LIME explained\n",
    "        exp_map = exp.as_map()\n",
    "        cls_key = next(iter(exp_map))    # either 0 or 1\n",
    "        raw     = exp_map[cls_key]       # list of (feat_idx, weight)\n",
    "\n",
    "        # filter only the first 5 active dummies:\n",
    "        kept = 0\n",
    "        for feat_idx, wt in raw:\n",
    "            if row[feat_idx] == 1:\n",
    "                feat_name     = feature_names_oh[feat_idx]\n",
    "                agg_freq[feat_name] += 1\n",
    "                agg_wt[feat_name]   += wt\n",
    "                kept += 1\n",
    "                if kept == 5:\n",
    "                    break\n",
    "\n",
    "# Build your summary_df as before...\n",
    "total = len(medoids_oh)*len(seeds)\n",
    "rows = [{\n",
    "    \"feature\": feat,\n",
    "    \"pct_in_top5\": 100*freq/total,\n",
    "    \"mean_signed_weight\": agg_wt[feat]/freq\n",
    "} for feat,freq in agg_freq.items()]\n",
    "\n",
    "summary_df = (pd.DataFrame(rows)\n",
    "                .sort_values([\"pct_in_top5\",\"mean_signed_weight\"], ascending=False)\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "print(summary_df.head(15))\n",
    "text_utils.ensure_directory_exists(\"eval/lime_results\")\n",
    "summary_df.to_csv(\"eval/lime_results/lime_nn_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799234c-c8db-4a8d-9307-0619a56e8976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
